{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python degrtytaee/generate_data_degree_e2e.py -c config/config_degree_e2e_ace05e.json\n",
    "# python degree/train_degree_e2e.py -c config/config_degree_e2e_ace05e.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Function\\Anoconda3\\envs\\data\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import nltk\n",
    "import spacy\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt_tab')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "from transformers import BertTokenizer\n",
    "import multiprocessing\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_mention(word):\n",
    "    # black_list = ['from', 'an', 'a', 'the', 'to', 'during', 'on', 'after', 'as', 'near', 'before']\n",
    "    doc = nlp(word)\n",
    "    word_prop = [i.pos_ for i in doc]\n",
    "\n",
    "    if len(word_prop) == 1:\n",
    "        return word\n",
    "    if len(word_prop) == 2:\n",
    "        if word_prop[0] in ['DET', 'ADJ', 'NUM']:\n",
    "            word = ' '.join(word.split(' ')[1:])\n",
    "    elif len(word_prop) >= 2:\n",
    "        if word_prop[0] in ['DET', 'ADJ', 'NUM'] and word_prop[1] in ['DET', 'ADJ', 'NUM']:\n",
    "            word = ' '.join(word.split(' ')[2:])\n",
    "    # word_repl = ''\n",
    "    # for i in word.split():\n",
    "    #     if i not in black_list:\n",
    "    #         word_repl += i\n",
    "    return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_e = {}\n",
    "for fn in os.listdir('./meta_data/arg_roles/'):\n",
    "    if not fn.endswith('.txt'):\n",
    "        continue\n",
    "    with open('./meta_data/arg_roles/' + fn, 'r', encoding='utf-8') as f:\n",
    "        r_data = json.load(f)\n",
    "    for k in r_data:\n",
    "        e_t = (fn.split('.txt')[0] + \":\" + k).upper()\n",
    "        r_e[e_t] = {}\n",
    "        for role in r_data[k]:\n",
    "            r_e[e_t][role.replace('-Arg', '')] = r_data[k][role][0].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# for fn in os.listdir('./'):\n",
    "#     if fn.endswith('.json') and re.search('^data_\\d*?_', fn):\n",
    "#         with open('./' + fn, 'r', encoding='utf-8') as f:\n",
    "#             r_data = json.load(f)\n",
    "#         # print(fn)\n",
    "#         for k, v in r_data.items():\n",
    "#             data[k] = v\n",
    "#         #     print(k, len(v))\n",
    "#         # print()\n",
    "# with open('./checkpoint.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = load_json('./meta_data/event_dict_full.json')\n",
    "# practical_roles = load_pickle('./exp_data/practical_roles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_sen_7_29_7_37.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp_data/trigger_sout_top10.json', 'r', encoding='utf-8') as f:\n",
    "    trigger_pool = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_arg_record(sen, reco):\n",
    "    for e_id in reco:\n",
    "        for role in reco[e_id]['argument']:\n",
    "            for i, arg in enumerate(reco[e_id]['argument'][role]):\n",
    "                if re.search(arg.lower(), sen.lower(), re.S) and not re.search(arg, sen, re.S):\n",
    "                    indices = [[match.start(), match.end()] for match in re.finditer(arg.lower(), sen.lower())]\n",
    "                    start, end = indices[0]\n",
    "                    arg_repl = sen[start:end]\n",
    "                    reco[e_id]['argument'][role][i] = arg_repl\n",
    "    return reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_upper(sen, w, event):\n",
    "    w_lower = w.lower()\n",
    "    sen_lower = sen.lower()\n",
    "    if re.search(w, sen, re.S):\n",
    "        return sen\n",
    "    elif re.search(w_lower, sen_lower, re.S):\n",
    "        indices = [[match.start(), match.end()] for match in re.finditer(w.lower(), sen.lower())]\n",
    "        for start, end in indices:\n",
    "            sen = sen[:start] + w + sen[end:]\n",
    "        return sen\n",
    "    else:\n",
    "        return sen\n",
    "\n",
    "def parse_capital(sen, rec):\n",
    "    try:\n",
    "        event = rec['event']\n",
    "        tri = rec['trigger']\n",
    "        args = rec['argument']\n",
    "        if not re.search(tri, sen):\n",
    "            sen = search_upper(sen, tri, event)\n",
    "        for k, v in args.items():\n",
    "            for a in v:\n",
    "                sen = search_upper(sen, a, event)\n",
    "        \n",
    "        return sen\n",
    "    except:\n",
    "        # print('error:\\tindex.{}\\t{}'.format(sen, rec))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "for k, v in all_data.items():\n",
    "    for i, r in enumerate(all_data[k]):\n",
    "        sen = r['sentence'] \n",
    "        all_data[k][i]['sentence'] = {}\n",
    "        all_data[k][i]['sentence']['sentences'] = [sen]\n",
    "        pop_list = []\n",
    "        for event_id in all_data[k][i]['record']:\n",
    "            event = all_data[k][i]['record'][event_id]['event']\n",
    "            all_data[k][i]['record'][event_id]['argument'] = {}\n",
    "            try:\n",
    "                real_role = [role for role in event_dict[event][1]]\n",
    "            except:\n",
    "                pop_list.append(event_id)\n",
    "        for pop_event in pop_list:\n",
    "            _ = all_data[k][i]['record'].pop(pop_event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arglist(args):\n",
    "    new_arg = []\n",
    "    for arg in args:\n",
    "        if isinstance(arg, list):\n",
    "            new_arg.extend(arg)\n",
    "        else:\n",
    "            new_arg.append(arg)\n",
    "    return Series(new_arg).drop_duplicates().tolist()\n",
    "    \n",
    "for e in all_data:\n",
    "    for i in range(len(all_data[e])):\n",
    "        sen = all_data[e][i]['sentence']\n",
    "        record = all_data[e][i]['record']\n",
    "        for e_id in record:\n",
    "            for role in record[e_id]['argument']:\n",
    "                args = parse_arglist(record[e_id]['argument'][role])\n",
    "                record[e_id]['argument'][role] = args\n",
    "        all_data[e][i]['record'] = record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_event = {}\n",
    "for e in all_data:\n",
    "    sen_event[e] = []\n",
    "    for i in range(len(all_data[e])):\n",
    "        sen = all_data[e][i]['sentence']\n",
    "        record = all_data[e][i]['record']\n",
    "        for e_id in record:\n",
    "            for role in record[e_id]['argument']:\n",
    "                args = [cut_mention(arg) for arg in record[e_id]['argument'][role]]\n",
    "                # args = [arg for arg in record[e_id]['argument'][role]]\n",
    "                \n",
    "                record[e_id]['argument'][role] = args\n",
    "        sen_event[e].append([' '.join(sen['sentences']), record])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_sen_pair = []\n",
    "for e in sen_event:\n",
    "#     [sen, rec]\n",
    "    for i in range(len(sen_event[e])):\n",
    "        sentence = sen_event[e][i][0]\n",
    "        for record in sen_event[e][i][1]:\n",
    "            r = sen_event[e][i][1][record]\n",
    "            sentence = parse_capital(sentence, r)\n",
    "        rec = unify_arg_record(sentence, sen_event[e][i][1])\n",
    "        if sentence:\n",
    "            event_sen_pair.append({'sentence':sentence, 'record': rec})\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for ep in event_sen_pair:\n",
    "    sen = ep['sentence']\n",
    "    if len(re.findall('trigger', sen)) != len(ep['record']):\n",
    "        s += 1\n",
    "print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_sen_pair_refined = []\n",
    "for s_r in event_sen_pair:\n",
    "    sentence = s_r['sentence']\n",
    "    pop_list = []\n",
    "    for event_id in s_r['record']:\n",
    "        e = s_r['record'][event_id]['event']\n",
    "        trigger = s_r['record'][event_id]['trigger']\n",
    "        if not re.findall(trigger, sentence, flags=re.I):\n",
    "            pop_list.append(event_id)\n",
    "    for pop_event in pop_list:\n",
    "        _ = s_r['record'].pop(pop_event)\n",
    "\n",
    "    event_sen_pair_refined.append({'sentence':sentence, 'record': s_r['record']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_mis = {}\n",
    "arg_mis = {}\n",
    "for i, d in enumerate(event_sen_pair_refined):\n",
    "    try:\n",
    "        sen = d['sentence']\n",
    "        rec = d['record']\n",
    "        if not i in tri_mis.keys():\n",
    "            tri_mis[i] = []\n",
    "        if not i in arg_mis.keys():\n",
    "            arg_mis[i] = []\n",
    "        for _, e in rec.items():\n",
    "            tri = e['trigger']\n",
    "            args = e['argument']\n",
    "            event = e['event']\n",
    "            if not re.search(tri, sen):\n",
    "                tri_mis[i].append([tri, sen])\n",
    "            for k, v in args.items():\n",
    "                for a in v:\n",
    "                    if not re.search(a, sen):\n",
    "                        arg_mis[i].append({k:[v, sen]})\n",
    "                    if len(a.split()) > 4:\n",
    "                        # print({k:[v, sen]})\n",
    "                        arg_mis[i].append({k:[v, sen]})\n",
    "    except:\n",
    "        print('error:\\tindex.{}\\t{}'.format(i, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for k, v in tri_mis.items():\n",
    "    if v:\n",
    "        # print(v)\n",
    "        # print(event_sen_pair_refined[k])\n",
    "    \n",
    "        s+=1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "sum_ = 0\n",
    "for k, v in arg_mis.items():\n",
    "    if v:\n",
    "        # print(k, v)\n",
    "        # break\n",
    "        sum_ += 1\n",
    "    else:\n",
    "        temp_list.append(event_sen_pair_refined[k])\n",
    "event_sen_pair_refined = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_sen_pair_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(event_sen_pair_refined)):\n",
    "    es_p = event_sen_pair_refined[i]\n",
    "    for event_id in es_p['record']:\n",
    "        for e in r_e.keys(): \n",
    "            if e.endswith(es_p['record'][event_id]['event'].upper()):\n",
    "                event_sen_pair_refined[i]['record'][event_id]['event'] = e\n",
    "                break\n",
    "                \n",
    "eve_se_p_f = []\n",
    "for es_p in event_sen_pair_refined:\n",
    "    if len(es_p['record']) < 1 or len(es_p['record']) > 5:\n",
    "        continue\n",
    "    for event_id in es_p['record']:\n",
    "        if len(es_p['record'][event_id]['trigger'].split(' ')) > 1:\n",
    "            continue\n",
    "    eve_se_p_f.append(es_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(event_sen_pair_refined)):\n",
    "#     sen = event_sen_pair_refined[i]['sentence']\n",
    "#     r = event_sen_pair_refined[i]['record']['event_1']\n",
    "#     e = r['event']\n",
    "#     trigger = r['trigger']\n",
    "#     if len(trigger.split(' ')) < 2:\n",
    "#         continue\n",
    "#     p = 0\n",
    "#     for sub_t in trigger.split(' '):\n",
    "#         for t in trigger_pool[e]:\n",
    "#             if len(t.split(' ')) > 1:\n",
    "#                 continue\n",
    "#             if sub_t.upper() == t.upper():\n",
    "# #                 if len(t) < 3:\n",
    "# #                     print(s_data[e][i]['record'][r]['trigger'], '\\t', sub_t, '\\t', t)\n",
    "#                 event_sen_pair_refined[i]['record']['event_1']['trigger'] = sub_t\n",
    "#                 p = 1\n",
    "#                 break\n",
    "#         if p == 1:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnl = WordNetLemmatizer()\n",
    "# for i in range(len(eve_se_p_f)):\n",
    "#     es_p = eve_se_p_f[i]\n",
    "#     sen_l = es_p['sentence'].split(' ')\n",
    "#     trigger = es_p['record']['event_1']['trigger']\n",
    "#     for w in sen_l:\n",
    "#         if wnl.lemmatize(trigger.strip(), 'v') == wnl.lemmatize(w.strip('[]().,'), 'v'):\n",
    "#             eve_se_p_f[i]['record']['event_1']['trigger'] = w.strip('[]().,')\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for i in eve_se_p_f:\n",
    "#     sen = i['sentence']\n",
    "#     t = i['record']['event_1']['trigger']\n",
    "    \n",
    "#     if not len(re.findall(t, sen)):\n",
    "#         cnt += 1\n",
    "\n",
    "# cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_event(s):\n",
    "    s_l = s.strip().split(':')\n",
    "    f_e = s_l[0].capitalize()\n",
    "    s_ee = ''\n",
    "    for s_e in s_l[1].split('-'):\n",
    "        s_ee += s_e.capitalize() + '-'\n",
    "    return f_e + ':' + s_ee[:-1]\n",
    "\n",
    "def convert_format(input_data, bert_tokenizer, role_to_entity_types):\n",
    "    # 初始化BERT tokenizer\n",
    "    timestamp = f\"{random.randint(2000, 2022)}{random.randint(10,12):02}{random.randint(10,31):02}\"\n",
    "    doc_id = f\"CNN_ENG_{timestamp}_{random.randint(100000, 999999)}.1\"\n",
    "    wnd_id = f\"{doc_id}-4\"\n",
    "\n",
    "    # 处理原始tokens和子词分词\n",
    "    sentence = input_data[\"sentence\"]\n",
    "    raw_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # 生成pieces和token_lens\n",
    "    pieces, token_lens = [], []\n",
    "    for token in raw_tokens:\n",
    "        subwords = bert_tokenizer.tokenize(token)\n",
    "        pieces.extend(subwords)\n",
    "        token_lens.append(len(subwords))\n",
    "\n",
    "    # 构建entity_mentions\n",
    "    entity_mentions = []\n",
    "    used_texts = set()\n",
    "\n",
    "    # 遍历所有事件参数\n",
    "    for event in input_data[\"record\"].values():\n",
    "        for role, texts in event[\"argument\"].items():\n",
    "            for text in texts:\n",
    "                if text in used_texts:\n",
    "                    continue  # 避免重复创建相同text的实体\n",
    "                used_texts.add(text)\n",
    "\n",
    "                # 查找文本在tokens中的位置\n",
    "                text_tokens = text.split()\n",
    "                start = -1\n",
    "                for i in range(len(raw_tokens) - len(text_tokens) + 1):\n",
    "                    if raw_tokens[i:i+len(text_tokens)] == text_tokens:\n",
    "                        start = i\n",
    "                        end = i + len(text_tokens)\n",
    "                        break\n",
    "\n",
    "                if start == -1:\n",
    "                    continue  # 未找到对应位置\n",
    "\n",
    "                # 获取实体类型（取第一个匹配类型）\n",
    "                entity_type = role_to_entity_types.get(event['event'].upper()).get(role, [\"UNK\"])[0]\n",
    "\n",
    "                entity_mentions.append({\n",
    "                    \"id\": f\"{wnd_id}-E{len(entity_mentions)}\",\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"mention_type\": \"UNK\",\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "    # 构建event_mentions\n",
    "    event_mentions = []\n",
    "    for ev_idx, event in enumerate(input_data[\"record\"].values()):\n",
    "        arguments = []\n",
    "        for role, texts in event[\"argument\"].items():\n",
    "            for text in texts:\n",
    "                # 查找对应的实体\n",
    "                entity = next(\n",
    "                    (e for e in entity_mentions if e[\"text\"] == text),\n",
    "                    None\n",
    "                )\n",
    "                if entity:\n",
    "                    arguments.append({\n",
    "                        \"entity_id\": entity[\"id\"],\n",
    "                        \"text\": text,\n",
    "                        \"role\": role\n",
    "                    })\n",
    "\n",
    "        # 查找trigger位置\n",
    "        trigger_tokens = event[\"trigger\"].split()\n",
    "        trigger_start = -1\n",
    "        for i in range(len(raw_tokens) - len(trigger_tokens) + 1):\n",
    "            if raw_tokens[i:i+len(trigger_tokens)] == trigger_tokens:\n",
    "                trigger_start = i\n",
    "                trigger_end = i + len(trigger_tokens)\n",
    "                break\n",
    "\n",
    "        event_mentions.append({\n",
    "            \"event_type\": capitalize_event(event[\"event\"]),\n",
    "            \"id\": f\"{wnd_id}-EV{ev_idx}\",\n",
    "            \"trigger\": {\n",
    "                \"start\": trigger_start,\n",
    "                \"end\": trigger_end,\n",
    "                \"text\": event[\"trigger\"]\n",
    "            },\n",
    "            \"arguments\": arguments\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"wnd_id\": wnd_id,\n",
    "        \"entity_mentions\": entity_mentions,\n",
    "        \"relation_mentions\": [],\n",
    "        \"event_mentions\": event_mentions,\n",
    "        \"entity_coreference\": [],\n",
    "        \"event_coreference\": [],\n",
    "        \"tokens\": raw_tokens,\n",
    "        \"pieces\": pieces,\n",
    "        \"token_lens\": token_lens,\n",
    "        \"sentence\": sentence,\n",
    "        \"sentence_starts\": [0]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'After the elections concluded, the newly elected leaders attended a peace summit to address the ongoing fighting in the region, which had resulted in numerous injuries among civilians.',\n",
       "  'record': {'event_1': {'event': 'PERSONNEL:ELECT',\n",
       "    'trigger': 'elections',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'CONTACT:MEET', 'trigger': 'summit', 'argument': {}},\n",
       "   'event_3': {'event': 'CONFLICT:ATTACK',\n",
       "    'trigger': 'fighting',\n",
       "    'argument': {}},\n",
       "   'event_4': {'event': 'LIFE:INJURE',\n",
       "    'trigger': 'injuries',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'After winning the election, the newly appointed mayor was shot during a violent protest, just hours before he was scheduled to meet with city officials; meanwhile, the governor was calling other leaders to coordinate a response.',\n",
       "  'record': {'event_1': {'event': 'PERSONNEL:ELECT',\n",
       "    'trigger': 'Election',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'CONTACT:MEET', 'trigger': 'meet', 'argument': {}},\n",
       "   'event_3': {'event': 'CONFLICT:ATTACK', 'trigger': 'shot', 'argument': {}},\n",
       "   'event_4': {'event': 'PERSONNEL:START-POSITION',\n",
       "    'trigger': 'appointed',\n",
       "    'argument': {}},\n",
       "   'event_5': {'event': 'PERSONNEL:ELECT',\n",
       "    'trigger': 'election',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'After the election results were announced, the newly elected mayor met with local leaders to discuss community safety, but tensions escalated into a violent fight outside the town hall, leading to his abrupt resign the following week; meanwhile, in a separate case, the controversial acquittal of the accused sparked public outrage.',\n",
       "  'record': {'event_1': {'event': 'PERSONNEL:ELECT',\n",
       "    'trigger': 'Election',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'CONTACT:MEET', 'trigger': 'met', 'argument': {}},\n",
       "   'event_3': {'event': 'CONFLICT:ATTACK', 'trigger': 'fight', 'argument': {}},\n",
       "   'event_4': {'event': 'PERSONNEL:END-POSITION',\n",
       "    'trigger': 'resign',\n",
       "    'argument': {}},\n",
       "   'event_5': {'event': 'PERSONNEL:ELECT',\n",
       "    'trigger': 'election',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'During the interview, the activist discussed the growing movement demanding justice, while overseas the escalating war claimed countless lives, including the controversial death of a political prisoner who was later pardoned posthumously.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'Interview',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'CONFLICT:DEMONSTRATE',\n",
       "    'trigger': 'movement',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'CONFLICT:ATTACK', 'trigger': 'war', 'argument': {}},\n",
       "   'event_4': {'event': 'JUSTICE:EXECUTE', 'trigger': 'death', 'argument': {}},\n",
       "   'event_5': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'interview',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'The delegates will discuss climate policies at the summit, where a new environmental coalition was created, and Dr. Chen announced her joining the organization as its chief scientist.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'Discuss',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'created',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'CONTACT:MEET', 'trigger': 'discuss', 'argument': {}},\n",
       "   'event_4': {'event': 'CONTACT:MEET', 'trigger': 'summit', 'argument': {}}}},\n",
       " {'sentence': 'After the summit, the delegates met to discuss the newly formed environmental coalition, which would fill key positions with experts, while the recent convictions of corporate leaders for fraud underscored the need for transparency.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'met',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'formed',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'PERSONNEL:START-POSITION',\n",
       "    'trigger': 'fill',\n",
       "    'argument': {}},\n",
       "   'event_4': {'event': 'JUSTICE:CONVICT',\n",
       "    'trigger': 'convictions',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'The leaders will discuss climate policies at the summit, while the newly created environmental coalition announced lawsuits against major polluters; meanwhile, the court found the defendant not guilty, and the mayor decided to appoint a new sustainability advisor to her administration.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'Discuss',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'created',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'JUSTICE:ACQUIT', 'trigger': 'guilty', 'argument': {}},\n",
       "   'event_4': {'event': 'JUSTICE:SUE', 'trigger': 'lawsuits', 'argument': {}},\n",
       "   'event_5': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'discuss',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'During their annual summit, the leaders held a crucial meeting to discuss global policies, and shortly after, they started a new environmental coalition; meanwhile, the president forgiven several political prisoners, and the delegates continued their meetings to finalize the trade agreement.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'meeting',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'started',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'started',\n",
       "    'argument': {}},\n",
       "   'event_4': {'event': 'JUSTICE:PARDON',\n",
       "    'trigger': 'forgiven',\n",
       "    'argument': {}},\n",
       "   'event_5': {'event': 'CONTACT:MEET', 'trigger': 'summit', 'argument': {}}}},\n",
       " {'sentence': 'During the peace talks, the delegates agreed to create a new humanitarian organization, just weeks after the former leader was convicted of war crimes.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'talks',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'create',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'JUSTICE:CONVICT',\n",
       "    'trigger': 'convicted',\n",
       "    'argument': {}}}},\n",
       " {'sentence': 'After the activists met to discuss human rights violations, they decided to form a new advocacy group; meanwhile, the notorious hacker was sent to prison following his arrest, though he was later released on parole, and now faces extradition to another country for cybercrime charges.',\n",
       "  'record': {'event_1': {'event': 'CONTACT:MEET',\n",
       "    'trigger': 'met',\n",
       "    'argument': {}},\n",
       "   'event_2': {'event': 'BUSINESS:START-ORG',\n",
       "    'trigger': 'form',\n",
       "    'argument': {}},\n",
       "   'event_3': {'event': 'JUSTICE:ARREST-JAIL',\n",
       "    'trigger': 'prison',\n",
       "    'argument': {}},\n",
       "   'event_4': {'event': 'JUSTICE:RELEASE-PAROLE',\n",
       "    'trigger': 'released',\n",
       "    'argument': {}},\n",
       "   'event_5': {'event': 'JUSTICE:ARREST-JAIL',\n",
       "    'trigger': 'arrest',\n",
       "    'argument': {}}}}]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#同位语修饰词，定语共指 -》 删除complexity control， 增加一事件比重\n",
    "#不合理的搭配 in iraq, Anwar submitted his resignation from United States\n",
    "# chain -> 提高arg f1, 降低trigger f1\n",
    "# constrain -> 有效\n",
    "# faiss sen * 10 + chain -> 略微降低arg f1, 略微提高trigger f1(0.5)\n",
    "# faiss sens[:4] + chain -> trigger f1 + 5, arg f1 - 1，无效\n",
    "# 核心 -》 有效记录 -》 模板生成 + faiss ? 约束解码训练？\n",
    "# 'Org': ['Stop n Shop'] -> 不合理arg\n",
    "\n",
    "# to do -> 1.删除complexity control，chain，添加faiss sen * 10 2.增加一事件比重\n",
    "\n",
    "# 提升：1.抽取\n",
    "#      2.constrain\n",
    "#      3.模板生成\n",
    "eve_se_p_f[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 1598/1598 [00:07<00:00, 220.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试运行\n",
    "# 示例映射字典\n",
    "\n",
    "\n",
    "ROLE_ENTITY_MAPPING = r_e\n",
    "train_data = eve_se_p_f\n",
    "\n",
    "data = []\n",
    "cnt = 0\n",
    "\n",
    "for input_data in tqdm(train_data, desc='Processing'):\n",
    "    try:\n",
    "        output = convert_format(input_data, bert_tokenizer, ROLE_ENTITY_MAPPING)\n",
    "        data.append(output)\n",
    "    except:\n",
    "        # print(input_data)\n",
    "        # break\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0.10262828535669574,\n",
       " 2: 0.08948685857321648,\n",
       " 3: 0.30788485607008875,\n",
       " 4: 0.30100125156445645,\n",
       " 5: 0.1989987484355437,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {i:0 for i in range(10)}\n",
    "for r in eve_se_p_f:\n",
    "    if len(r['record']) < 1:\n",
    "        print(r)\n",
    "    d[len(r['record'])] += 1 / len(eve_se_p_f)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1479"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exp_data/train.w1.oneie.json', 'w', encoding='utf-8') as f1:\n",
    "    for l in data:\n",
    "        d = json.dumps(l)\n",
    "        f1.write(d + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'CNN_ENG_20101123_281932.1',\n",
       " 'wnd_id': 'CNN_ENG_20101123_281932.1-4',\n",
       " 'entity_mentions': [],\n",
       " 'relation_mentions': [],\n",
       " 'event_mentions': [{'event_type': 'Personnel:Elect',\n",
       "   'id': 'CNN_ENG_20101123_281932.1-4-EV0',\n",
       "   'trigger': {'start': 2, 'end': 3, 'text': 'elections'},\n",
       "   'arguments': []},\n",
       "  {'event_type': 'Contact:Meet',\n",
       "   'id': 'CNN_ENG_20101123_281932.1-4-EV1',\n",
       "   'trigger': {'start': 12, 'end': 13, 'text': 'summit'},\n",
       "   'arguments': []},\n",
       "  {'event_type': 'Conflict:Attack',\n",
       "   'id': 'CNN_ENG_20101123_281932.1-4-EV2',\n",
       "   'trigger': {'start': 17, 'end': 18, 'text': 'fighting'},\n",
       "   'arguments': []},\n",
       "  {'event_type': 'Life:Injure',\n",
       "   'id': 'CNN_ENG_20101123_281932.1-4-EV3',\n",
       "   'trigger': {'start': 27, 'end': 28, 'text': 'injuries'},\n",
       "   'arguments': []}],\n",
       " 'entity_coreference': [],\n",
       " 'event_coreference': [],\n",
       " 'tokens': ['After',\n",
       "  'the',\n",
       "  'elections',\n",
       "  'concluded',\n",
       "  ',',\n",
       "  'the',\n",
       "  'newly',\n",
       "  'elected',\n",
       "  'leaders',\n",
       "  'attended',\n",
       "  'a',\n",
       "  'peace',\n",
       "  'summit',\n",
       "  'to',\n",
       "  'address',\n",
       "  'the',\n",
       "  'ongoing',\n",
       "  'fighting',\n",
       "  'in',\n",
       "  'the',\n",
       "  'region',\n",
       "  ',',\n",
       "  'which',\n",
       "  'had',\n",
       "  'resulted',\n",
       "  'in',\n",
       "  'numerous',\n",
       "  'injuries',\n",
       "  'among',\n",
       "  'civilians',\n",
       "  '.'],\n",
       " 'pieces': ['After',\n",
       "  'the',\n",
       "  'elections',\n",
       "  'concluded',\n",
       "  ',',\n",
       "  'the',\n",
       "  'newly',\n",
       "  'elected',\n",
       "  'leaders',\n",
       "  'attended',\n",
       "  'a',\n",
       "  'peace',\n",
       "  'summit',\n",
       "  'to',\n",
       "  'address',\n",
       "  'the',\n",
       "  'ongoing',\n",
       "  'fighting',\n",
       "  'in',\n",
       "  'the',\n",
       "  'region',\n",
       "  ',',\n",
       "  'which',\n",
       "  'had',\n",
       "  'resulted',\n",
       "  'in',\n",
       "  'numerous',\n",
       "  'injuries',\n",
       "  'among',\n",
       "  'civilians',\n",
       "  '.'],\n",
       " 'token_lens': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'sentence': 'After the elections concluded, the newly elected leaders attended a peace summit to address the ongoing fighting in the region, which had resulted in numerous injuries among civilians.',\n",
       " 'sentence_starts': [0]}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = data\n",
    "# sen_cnt = 0\n",
    "# event_data = {}\n",
    "# c_m = 0\n",
    "# for data_ in train:\n",
    "#     # if len(data['event_mentions']) > 1:\n",
    "#     #     c_m += 1\n",
    "#     #     e_type = e['event_type']\n",
    "\n",
    "#     for e in data_['event_mentions']:\n",
    "#         e_type = e['event_type']\n",
    "#         sen_cnt += 1\n",
    "#         if not event_data.get(e_type):\n",
    "#             event_data[e_type] = []\n",
    "#         for info in e['arguments']:\n",
    "#             if info['role'] not in event_data[e_type]:\n",
    "#                 event_data[e_type].append(info['role'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data]",
   "language": "python",
   "name": "conda-env-data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
